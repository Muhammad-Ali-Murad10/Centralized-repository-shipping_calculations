{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrqTcKlO3Eqz"
      },
      "source": [
        "Shows how one can generate text given a prompt and some hyperparameters, using either minGPT or huggingface/transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/karpathy/minGPT.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYH9RfiX6sAa",
        "outputId": "03d67cd6-76d4-4be4-d866-2a4c80f9f34b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/karpathy/minGPT.git\n",
            "  Cloning https://github.com/karpathy/minGPT.git to /tmp/pip-req-build-z709va26\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/karpathy/minGPT.git /tmp/pip-req-build-z709va26\n",
            "  Resolved https://github.com/karpathy/minGPT.git to commit 37baab71b9abea1b76ab957409a1cc2fbfba8a26\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from minGPT==0.0.1) (2.6.0+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->minGPT==0.0.1) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->minGPT==0.0.1) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->minGPT==0.0.1) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->minGPT==0.0.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->minGPT==0.0.1) (2025.7.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->minGPT==0.0.1) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->minGPT==0.0.1) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->minGPT==0.0.1) (3.0.2)\n",
            "Building wheels for collected packages: minGPT\n",
            "  Building wheel for minGPT (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for minGPT: filename=mingpt-0.0.1-py3-none-any.whl size=15435 sha256=e4c75aad8c9b06824b6634a0a7e1395ba41e4e4074ac66774c8f87e1704c2013\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5xx4a_a8/wheels/eb/8b/dc/d67c2183400e22b659530b4e46225da5a2da455725afe4a90a\n",
            "Successfully built minGPT\n",
            "Installing collected packages: minGPT\n",
            "Successfully installed minGPT-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "unck31yP3Eq7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from mingpt.model import GPT\n",
        "from mingpt.utils import set_seed\n",
        "from mingpt.bpe import BPETokenizer\n",
        "set_seed(3407)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "9NQ8zdsE3ErA"
      },
      "outputs": [],
      "source": [
        "use_mingpt = True # use minGPT or huggingface/transformers model?\n",
        "model_type = 'gpt2-xl'\n",
        "device = 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "WFI0oO963ErC"
      },
      "outputs": [],
      "source": [
        "# if use_mingpt:\n",
        "#     model = GPT.from_pretrained(model_type)\n",
        "# else:\n",
        "model = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "model.config.pad_token_id = model.config.eos_token_id # suppress a warning\n",
        "\n",
        "# ship model to device and set to eval mode\n",
        "model.to(device)\n",
        "model.eval();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "SlTjKJzh3ErF"
      },
      "outputs": [],
      "source": [
        "\n",
        "def generate(prompt='', num_samples=10, steps=20, do_sample=True):\n",
        "\n",
        "    # tokenize the input prompt into integer input sequence\n",
        "    if use_mingpt:\n",
        "        tokenizer = BPETokenizer()\n",
        "        if prompt == '':\n",
        "            # to create unconditional samples...\n",
        "            # manually create a tensor with only the special <|endoftext|> token\n",
        "            # similar to what openai's code does here https://github.com/openai/gpt-2/blob/master/src/generate_unconditional_samples.py\n",
        "            x = torch.tensor([[tokenizer.encoder.encoder['<|endoftext|>']]], dtype=torch.long)\n",
        "        else:\n",
        "            x = tokenizer(prompt).to(device)\n",
        "    else:\n",
        "        tokenizer = GPT2Tokenizer.from_pretrained(model_type)\n",
        "        if prompt == '':\n",
        "            # to create unconditional samples...\n",
        "            # huggingface/transformers tokenizer special cases these strings\n",
        "            prompt = '<|endoftext|>'\n",
        "        encoded_input = tokenizer(prompt, return_tensors='pt').to(device)\n",
        "        x = encoded_input['input_ids']\n",
        "\n",
        "    # we'll process all desired num_samples in a batch, so expand out the batch dim\n",
        "    x = x.expand(num_samples, -1)\n",
        "\n",
        "    # forward the model `steps` times to get samples, in a batch\n",
        "    y = model.generate(x, max_new_tokens=steps, do_sample=do_sample, top_k=40)\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        out = tokenizer.decode(y[i].cpu().squeeze())\n",
        "        print('-'*80)\n",
        "        print(out)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1rT92LA3ErG",
        "outputId": "d48e308e-3bca-450b-ade6-2a81318b548a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Andrej Karpathy, the head of the Center for Internet Policy at the Hungarian Academy of Sciences' National University and one of the\n",
            "--------------------------------------------------------------------------------\n",
            "Andrej Karpathy, the developer of the system. He says he's already received several emails from people who said they now use\n",
            "--------------------------------------------------------------------------------\n",
            "Andrej Karpathy, the general counsel, said that the company plans to review the company's legal positions when the matter is fully\n",
            "--------------------------------------------------------------------------------\n",
            "Andrej Karpathy, the chief executive of the Centre for Economic Studies, said: \"We have been predicting a decline of around\n",
            "--------------------------------------------------------------------------------\n",
            "Andrej Karpathy, the executive director of the Washington Center for Equitable Growth at UW, had a more direct, if grim\n",
            "--------------------------------------------------------------------------------\n",
            "Andrej Karpathy, the Russian president's press secretary, told reporters that the two men had been \"interacting with each other\n",
            "--------------------------------------------------------------------------------\n",
            "Andrej Karpathy, the man at fault for the incident, is a member of the Russian national football team.<|endoftext|><|endoftext|><|endoftext|>\n",
            "--------------------------------------------------------------------------------\n",
            "Andrej Karpathy, the Russian-born co-founder of the firm that eventually became Atlassian, was an early investor in\n",
            "--------------------------------------------------------------------------------\n",
            "Andrej Karpathy, the city's current planning commissioner, said he was concerned about the impact of a new law and the potential\n",
            "--------------------------------------------------------------------------------\n",
            "Andrej Karpathy, the leader of the center-left opposition Social Democrats, said that \"a political agreement between President [V\n"
          ]
        }
      ],
      "source": [
        "generate(prompt='Andrej Karpathy, the', num_samples=10, steps=20)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5988099e"
      },
      "source": [],
      "execution_count": 3,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}